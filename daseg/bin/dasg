#!/usr/bin/env python
import pickle
import sys
from pathlib import Path
from typing import Optional

import click
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from torch.utils.data.dataloader import DataLoader

from daseg import DialogActCorpus, TransformerModel
from daseg.data import BLANK
from daseg.dataloaders.turns import SingleTurnDataset, padding_collate_fn
from daseg.models.bigru import ZhaoKawaharaBiGru
from daseg.slack import SlackNotifier


@click.group()
def cli():
    pass


@cli.command()
@click.argument('output_dir', type=click.Path())
@click.option('--dataset-path', default='deps/swda/swda', type=click.Path(exists=True))
@click.option('--window-size', type=int, default=None)
@click.option('--window-overlap', type=int, default=None)
@click.option('-p', '--strip-punctuation-and-lowercase', is_flag=True)
@click.option('-c', '--continuations-allowed', is_flag=True)
@click.option('-t', '--window-test', is_flag=True)
@click.option('-s', '--tagset', default='basic',
              type=click.Choice(['basic', 'general', 'full', 'custom', 'segmentation']))
@click.option('-j', '--no-joint-coding', is_flag=True)
def prepare_data(
        output_dir: str,
        dataset_path: str,
        window_size: Optional[int],
        window_overlap: Optional[int],
        strip_punctuation_and_lowercase: bool,
        continuations_allowed: bool,
        window_test: bool,
        tagset: str,
        no_joint_coding: bool
):
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    dataset = DialogActCorpus.from_path(
        dataset_path=dataset_path,
        strip_punctuation_and_lowercase=strip_punctuation_and_lowercase,
        tagset=tagset
    )
    for split_name, split_dataset in dataset.train_dev_test_split().items():
        split_dataset.dump_for_transformers_ner(
            output_dir / f'{split_name}.txt.tmp',
            acts_count_per_sample=window_size if split_name != 'test' or window_test else None,
            acts_count_overlap=window_overlap if split_name != 'test' or window_test else None,
            continuations_allowed=continuations_allowed,
            use_joint_coding=not no_joint_coding
        )


@cli.command()
@click.argument('model_path', type=click.Path(exists=True, file_okay=False))
@click.option('--dataset-path', default='deps/swda/swda', type=click.Path(exists=True))
@click.option('--split', type=click.Choice(['train', 'dev', 'test']), default='test')
@click.option('--batch_size', default=4, type=int)
@click.option('--window_len', default=None, type=int)
@click.option('--device', default='cpu', type=str)
@click.option('-o', '--save-output', default=None, type=click.Path())
@click.option('-p', '--strip-punctuation-and-lowercase', is_flag=True)
@click.option('-r', '--begin-determines-act', is_flag=True)
@click.option('-s', '--tagset', default='basic',
              type=click.Choice(['basic', 'general', 'full', 'custom', 'segmentation']))
@click.option('-d', '--dont-propagate-context', is_flag=True)
@click.option('-v', '--verbose', is_flag=True)
@click.option('-j', '--no-joint-coding', is_flag=True)
def evaluate(
        model_path: str,
        dataset_path: str,
        split: str,
        batch_size: int,
        window_len: int,
        device: str,
        save_output: Optional[str],
        strip_punctuation_and_lowercase: bool,
        begin_determines_act: bool,
        tagset: str,
        dont_propagate_context: bool,
        verbose: bool,
        no_joint_coding: bool
):
    dataset = DialogActCorpus.from_path(
        dataset_path,
        splits=[split],
        strip_punctuation_and_lowercase=strip_punctuation_and_lowercase,
        tagset=tagset
    )
    model = TransformerModel.from_path(Path(model_path), device=device)
    results = model.predict(
        dataset=dataset,
        batch_size=batch_size,
        window_len=window_len,
        propagate_context=not dont_propagate_context,
        begin_determines_act=begin_determines_act,
        verbose=verbose,
        use_joint_coding=not no_joint_coding
    )
    with SlackNotifier(' '.join(sys.argv)) as slack:
        for res_grp in ('sklearn_metrics', 'seqeval_metrics', 'zhao_kawahara_metrics'):
            slack.write_and_print(f'{res_grp.upper()}:')
            for key, val in results[res_grp].items():
                slack.write_and_print(f'{key}\t{val:.2%}')
    if save_output is not None:
        with open(save_output, 'wb') as f:
            pickle.dump(results, f)


@cli.command()
@click.argument('output-dir', type=click.Path())
@click.option('--dataset-path', default='deps/swda/swda', type=click.Path(exists=True))
@click.option('--batch-size', default=30, type=int)
@click.option('-p', '--strip-punctuation-and-lowercase', is_flag=True)
@click.option('-s', '--tagset', default='basic',
              type=click.Choice(['basic', 'general', 'full', 'custom', 'segmentation']))
@click.option('-g', '--num-gpus', default=0, type=int)
def train_bigru(output_dir, dataset_path, batch_size, strip_punctuation_and_lowercase, tagset, num_gpus):
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    pl.seed_everything(1050)
    corpus = DialogActCorpus.from_path(
        dataset_path,
        strip_punctuation_and_lowercase=strip_punctuation_and_lowercase,
        tagset=tagset
    ).with_limited_vocabulary(10000)
    word2idx = {w: i + 1 for i, w in enumerate(corpus.vocabulary.keys())}
    tag2idx = {t: i for i, t in enumerate(t for t in corpus.joint_coding_dialog_act_labels if t != BLANK)}
    with open(output_path / 'word2idx', 'w') as f:
        for k, v in word2idx.items(): print(f'{k} {v}', file=f)
    with open(output_path / 'tag2idx', 'w') as f:
        for k, v in tag2idx.items(): print(f'{k} {v}', file=f)
    loaders = {
        key: DataLoader(
            dataset=SingleTurnDataset(split_corpus, word2idx=word2idx, tag2idx=tag2idx),
            batch_size=batch_size,
            shuffle=key == 'train',
            collate_fn=padding_collate_fn
        )
        for key, split_corpus in corpus.train_dev_test_split().items()
    }
    model = ZhaoKawaharaBiGru(vocab=word2idx, labels=tag2idx)
    trainer = pl.Trainer(
        gradient_clip_val=5.0,  # note: not necessarily the right method of gradient clipping
        default_root_dir=output_dir,
        gpus=num_gpus,
        deterministic=True,
        checkpoint_callback=ModelCheckpoint(
            filepath=output_dir,
            verbose=True,
            monitor='val_loss',
            mode='min'
        ),
        max_epochs=10
    )
    trainer.fit(
        model=model,
        train_dataloader=loaders['train'],
        val_dataloaders=loaders['dev']
    )
    trainer.test(
        model=model,
        test_dataloaders=loaders['test']
    )


if __name__ == '__main__':
    cli()
